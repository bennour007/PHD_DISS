# The interconnectedness between regions and technologies

## Methodological Motivation

Traditional relatedness frameworks model diversification through symmetric co-occurrence matrices and linear aggregation of relatedness density [@hausmann2011network]. However, as established in our problem statement, these constructs suffer from three structural limitations: (1) symmetry assumptions that obscure directional dependencies between technologies, (2) linear aggregation that loses granular information about technology-specific requirements, and (3) noise when technologies outnumber regions. 

We address these limitations by replacing traditional measures with a machine learning training strategy based on the Random Forest algorithm (RF). Specifically, we use RF to generate: (1) **Feature Importance Technology Space (FITS)** follwoing @fessina2024identifying, analogous to the mainstream Technology Space. FITS captures directional, hierarchical technology relationships, replacing symmetric relatedness measures; and (2) **predicted probabilities (technological potential)** following @albora2023product, analogous to relatedness density. The technological potential estimates region-specific feasibility of technology adoption given the region's own technological portfolio, replacing linear relatedness density. This approach enables the contextualization of diversification strategies by accounting simultaneously for technology-specific characteristics, regional knowledge infrastructure, and the broader non-linear interconnectedness between regions and technologies that traditional measures cannot adequately capture.

Before detailing our methodology and how we leverage the contributions in @fessina2024identifying and @albora2023product, we will briefly situate this approach within the literature namely The Relatedness and Economic Complexity (REC). Essentially, REC formalizes the empirical observation that shared input requirements (knowledge, resources, capabilities) determine diversification feasibility [@hidalgo2018principle; @hidalgo2021economic]. Economic Complexity (EC) complements this by quantifying sophistication patterns [@hidalgo2009building]. While this framework have proven valuable for policy [@zaccaria2018integrating; @albora2021economic], deriving granular, context-specific implications remains challenging [@hidalgo2023policy; @li2024evaluating]. Recent work on unrelated diversification [@pinheiro2022time; @boschma2023role], geographic inequalities [@pinheiro2025dark; @hartmann2017linking], and emerging technologies [@lee2018early; @fessina2024identifying] highlights the need for methodologies that capture contextual nuances beyond path dependency identification.

Our approach responds to this need by modeling technology relationships and regional capabilities in ways that explicitly incorporate heterogeneity. The following sections establish our notation system, details the mainstream approach to measuring relatedness and relatedness density, describe the Random Forest algorithm we use to our aim, and detail how we construct and interpret technological potential, FITS, and coherence measures.


We establish the following notation used throughout this work:


- $\mathcal{T} = \{t_i\}_{i=1}^{N_T}$: set of technologies
- $\mathcal{R} = \{r_i\}_{i=1}^{N_R}$: set of regions  
- $\mathcal{Y} = \{y_i\}_{i=1}^{N_Y}$: set of years
- $\mathcal{C} = \{c_i\}_{i=1}^{N_C}$: set of technology categories
- $\mathcal{K} = \{\kappa_i\}_{i=1}^{N_K}$: set of countries



We use European Patent Office data (1978-2021) classified at the 4-digit IPC level, yielding 641 distinct technologies across 345 NUTS2 regions in 34 European countries. IPC classifications provide hierarchical structure: section (letter, we also refer to this as categories), class (two digits), subclass (letter). For example, F16H encompasses Section F (Mechanical engineering), Class 16 (engineering elements for mechanical power transmission), and Subclass H (gearing systems). We supplement patent data with Eurostat regional socio-economic indicators detailed in subsequent sections.

We quantify regional specialization using the Revealed Comparative Advantage [@balassa1965rca]. Although originally designed for trade data, this metric has been widely adopted in innovation geography literature. Following @balland2017geography, we refer to it as Revealed Technological Advantage (RTA) in our patent context. The RTA measures relative specialization, enabling simultaneous capture of expertise depth and portfolio diversity. Despite critiques regarding patent-based applications [@balland2019beyond; @diodato2023economic], RTA aligns with our objective of capturing meaningful technology relationships through machine learning rather than raw co-occurrence.

The RTA for region $r$ in technology $t$ during year $y$ is:

$$
\text{RTA}_{r,t,y}
= \frac{\displaystyle\frac{X_{r,t,y}}{\sum_{t'} X_{r,t',y}}}
       {\displaystyle\frac{\sum_{r'} X_{r',t,y}}{\sum_{r',t'} X_{r',t',y}}}
= \frac{X_{r,t,y}\,\sum_{r',t'}X_{r',t',y}}
       {\bigl(\sum_{t'}X_{r,t',y}\bigr)\,\bigl(\sum_{r'}X_{r',t,y}\bigr)}
$$
Where, $X_{r,t,y}$: patent count for region $r$ in technology $t$ during year $y$

For each year $y$, we construct the RTA matrix $\mathbf{R}^{(y)}$ with entries $\text{RTA}_{r,t,y}$:

$$
\mathbf{R}^{(y)}
= \bigl[\text{RTA}_{r,t,y}\bigr]_{r=1,\dots,N_{R}}^{t=1,\dots,N_{T}}
$$

These yearly matrices form the foundation for all subsequent modeling. We then binarize specialization for classification tasks:

$$
z_{r,t,y}
\;=\;
\begin{cases}
1 & \text{if }\text{RTA}_{r,t,y}\ge1 \\
0 & \text{otherwise}
\end{cases}
$$

where $z_{r,t,y} = 1$ indicates region $r$ has comparative advantage (specialization) in technology $t$ at year $y$.

To further elaborate, let's consider $c_{t,t'}$, the count of patents containing both technologies $t$ and $t'$ ($t\neq t'$) at a given time period, such that $t, t' \in \mathcal{T}$, and $c_{t}$, $c_{t'}$ denote the total patent counts for each class individually.

In the following we will begin by outlining how the REC literature approach relatedness, how it's calculated and discuss in detail different technical contributions to this end and the limitation of the mainstream approach. We will then present our approach and detail our methodological and conceptual contribution in response to these limitations. 


## Relatedness as measured via co-occurence

In the REC literature relatedness is calculated based on the pair-wise observed frequency of co-occurrences. The idea is simple: frequent co-occurrences mean strong links between technologies. Here we show in detail how relatedness is measured, and the different normalization techniques used to harmonize the pairwise links.

The simplest measure of relatedness here would be to consider $c_{t,t'}$. However, as we mentioned in the introduction, co-occurrences are noisy and harmonizing the pairwise values is crucial. To this end, the REC literature considers different normalization methods. Here we will briefly outline the most commonly used methods.

**Association Strength** 

$$
\phi_{t,t'}^{\text{assoc}} = \frac{c_{t,t'} \cdot T}{c_t \cdot c_{t'}}
$$
where $T = \sum_{t \in \mathcal{T}} c_t$ is the total patent count. This probabilistic measure developed in @eck2009normalize compares observed to expected co-occurrence under independence. Values above 1 indicate stronger-than-random association. It properly corrects for size effects.

**Probability Index**  

$$
\phi_{t,t'}^{\text{prob}} = \frac{c_{t,t'}}{\frac{1}{2}\left[\frac{c_t}{T} \cdot \frac{c_{t'}}{T - c_t} + \frac{c_{t'}}{T} \cdot \frac{c_t}{T - c_{t'}}\right] \cdot T}
$$
This improved probabilistic measure proposed by @steijn2017improvement uses combinations without repetition, correcting for the fact that a patent cannot co-occur with itself. It provides more accurate estimates than association strength for patent co-occurrence analysis. This normalisation method is implemented in the EconGeo R package [@balland2016econgeo].

**Cosine Similarity** 

$$
\phi_{t,t'}^{\text{cosine}} = \frac{c_{t,t'}}{\sqrt{c_t \cdot c_{t'}}}
$$
A set-theoretic measure of relative overlap. While intuitive, it systematically favors frequent technologies over rare ones and does not properly correct for size effects.

**Minimum Conditional Probability** 

$$
\phi_{t,t'}^{\text{min}} = \min\left\{P(\text{RTA}_{r,t} | \text{RTA}_{r,t'}), P(\text{RTA}_{r,t'} | \text{RTA}_{r,t})\right\}
$$

where $\text{RTA}_{r,t}$ indicates that region $r \in \mathcal{R}$ has a revealed technological advantage in technology $t$. This approach-popularised by @hidalgo2007product-computes the minimum of pair-wise conditional probabilities of regions having RTA in one technology given they have RTA in another. 


**Relatedness Density**

Relatedness density, is a much simpler aggregation. Essentially it measures how close a region's existing technological portfolio is to a potential new technology. For region $r \in \mathcal{R}$, technology $t \in \mathcal{T}$, relatedness density aggregates the relatedness between technology $t$ and all technologies in which the region specializes($\mathcal{T}_r$) [@hidalgo2007product; @boschma2015relatedness]:
$$
\text{relatedness density}: \Phi_{r,t} = \frac{\sum_{t \in \mathcal{T}_r \,  t' \neq t} \phi_{t,t'}}{\sum_{t' \neq t} \phi_{t,t'}} \times 100
$$
The numerator sums relatedness values to technologies in which the region specializes, while the denominator normalizes by total relatedness to all technologies. High density indicates technology $t$ is "close" to the region's existing capabilities; low density suggests it requires capabilities the region lacks.





## Random Forest Algorithm

Before detailing our applications, we provide comprehensive context on the Random Forest algorithm, which fuels our entire methodological approach. We focus on binary classification where $y_i \in \{0,1\}$.



Random Forest constructs an ensemble of $B$ decision trees through bootstrap aggregating (bagging) with random feature subsampling. Each tree $T_b$ is built on a bootstrap sample $\mathcal{D}_b^*$ drawn with replacement from the original dataset.


Each tree recursively partitions the feature space through binary splits. At node $t$, we randomly select $m$ features (typically $m = \sqrt{p}$) and evaluate all possible splits within this subset. For feature $j$ and threshold $\tau$, the split creates two child nodes: $t_L = \{i : x_{ij} \leq \tau\}$ and $t_R = \{i : x_{ij} > \tau\}$.


Split quality is assessed via Gini impurity:

$$G(t) = 1 - \sum_{k=0}^{1} p_k^2(t) = 2p_0(t)p_1(t)$$

where $p_k(t) = n_k(t)/n(t)$ represents the proportion of class $k$ observations at node $t$. Gini impurity quantifies node heterogeneity: $G=0$ indicates perfect purity (homogeneous class), while $G=0.5$ indicates maximum impurity (equal class distribution). The optimal split maximizes weighted impurity reduction:

$$\Delta G(j, \tau) = G(t) - \left[\frac{n(t_L)}{n(t)} G(t_L) + \frac{n(t_R)}{n(t)} G(t_R)\right]$$

Weighting by relative node size prevents trivial splits that isolate single observations into pure but uninformative leaves.


Recursive splitting continues until predefined stopping criteria: node purity ($G=0$), minimum node size threshold, or maximum tree depth. Terminal nodes are assigned the majority class of their constituent observations.


For prediction, observation $\mathbf{x}$ traverses all $B$ trees. Final classification aggregates individual tree predictions via majority voting:

$$\hat{y}(\mathbf{x}) = \text{mode}\{\hat{y}_1(\mathbf{x}), \ldots, \hat{y}_B(\mathbf{x})\}$$

Class probabilities are estimated as the proportion of trees predicting each class:

$$\hat{P}(y=1|\mathbf{x}) = B^{-1}\sum_{b=1}^{B} \mathbb{I}[\hat{y}_b(\mathbf{x})=1]$$

This probability represents empirical vote share across trees. Values near 1 indicate strong consensus for class 1 (high confidence), while values near 0.5 reflect uncertainty. Unlike parametric models, these are data-driven vote proportions rather than model-based probability estimates.



The algorithm's effectiveness stems from variance reduction through decorrelated predictions. Bootstrap sampling and random feature selection reduce inter-tree correlation $\rho$, yielding ensemble variance:

$$\text{Var}(\bar{y}) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2$$

As $B$ increases and $\rho$ decreases, ensemble variance diminishes while maintaining the low bias of flexible tree models.


Feature importance quantifies each predictor's contribution by aggregating Gini impurity reductions:

$$I(j) = \frac{1}{B}\sum_{b=1}^{B} \sum_{t \in T_b : v(t)=j} \Delta G(t)$$

where the sum runs over all nodes using feature $j$ for splitting. Higher values indicate features consistently creating purer partitions. Feature importance measures predictive association rather than causal effect, and suffers from bias toward high-cardinality features and correlated predictor sets. It ranks predictive relevance but requires caution in causal interpretation.

![](resources/ML_relatedness_tachella.jpg)


## Connecting Random Forest to the Relatedness Framework


### Machine learning as an alternative

Our Random Forest approach addresses these by: (1) extracting asymmetric dependencies through directed FITS edges, (2) capturing non-linear interactions through tree-based splitting, and (3) leveraging temporal dynamics and cross-regional variation to overcome sparse co-occurrence patterns.

We apply Random Forest in two complementary ways that replace traditional relatedness constructs:

**A. Technological Potential** estimates $p_{r,t,y}$ = probability of future specialization in technology $t$ for region $r$. This replaces **relatedness density**—instead of linearly aggregating symmetric co-occurrence frequencies, we predict region-specific feasibility using past technology portfolios. This captures non-linear interactions and regional heterogeneity that linear measures miss.

**B. Feature Importance Technology Space (FITS)** extracts $I_{t \to t'}$ = directional dependencies between technologies. This replaces **symmetric relatedness**—instead of co-occurrence frequencies, we identify which technologies predict others' future adoption. Asymmetry reveals hierarchical structures: if $I_{t \to t'} \gg I_{t' \to t}$, technology $t$ is a prerequisite or "stepping stone" toward $t'$.

Together, these measures enable contextualized diversification analysis: FITS reveals technology-level hierarchies, Potential quantifies region-specific feasibility, and their combination allows testing how regional infrastructure, national ecosystems, and spatial factors moderate technology adoption patterns.


```{mermaid}

graph TB
    A[Input: RCA values<br/>for all technologies<br/>except target] --> B[Tree 1]
    A --> C[Tree 2]
    A --> D[Tree 3]
    A --> E[...]
    A --> F[Tree B]
    
    B --> G[Vote: 1]
    C --> H[Vote: 0]
    D --> I[Vote: 1]
    E --> J[Vote: 1]
    F --> K[Vote: 1]
    
    G --> L[Aggregate Votes]
    H --> L
    I --> L
    J --> L
    K --> L
    
    L --> M[Probability = 4/5 = 0.80<br/>Region will specialize<br/>in target technology]
    
    style A fill:#e1f5ff
    style M fill:#ffe1e1

```





### Training Procedure


We follow the methodology of @fessina2024identifying and @albora2023product, originally developed for trade data. The innovation lies in modeling each technology separately rather than constructing a single global model. For every technology $i \in \mathcal{T}$, we train a binary classification model where:

- **Outcome**: $z_{r,i,y}$ (whether region $r$ specializes in technology $i$ at year $y$)
- **Features**: $\{\text{RTA}_{r,t,y-\delta} : t \in \mathcal{T}, t \neq i\}$ (RTA values for all other technologies $\delta$ years prior)

This technology-specific approach enables each model to learn unique dependency patterns. Setting $\delta = 4$ years balances predictive performance with data availability such that current capacity predict future ones [@andreoni2019political]. 


We predict specialization for target years $y_t \in \{2008, \ldots, 2018\}$ using data from 1978 onward. For each target year $y_t$ and technology $i$:

**Training set:**
$$
X_{\text{train}} = \{ \text{RTA}_{r,t,y} \mid y \in [1978, y_t - 2\delta], t \neq i\}
$$
$$
Y_{\text{train}} = \{z_{r,i,y} \mid y \in [1978 + \delta,  y_t - \delta]\}
$$

**Test set:**
$$
X_{\text{test}} = \{ \text{RTA}_{r,t,y_t-\delta} \mid t \neq i\}
$$
$$
Y_{\text{test}} = \{z_{r,i,y_t}\}
$$

This produces 7,051 models (641 technologies × 11 years). Given computational constraints, we performed cross-validation on a random sample of four technologies (G06G, B67B, D02J, C08J) and applied the most frequent optimal parameters across all models: `mtry = 139`, `trees = 100`, `min_n = 38`. Training was conducted in R using the Ranger package [@wright2017ranger], orchestrated via the targets pipeline [@landau2021targets] within the tidymodels framework.



## Technological Potential


Each model produces probabilities $p_{r,i,y} = P(z_{r,i,y} = 1 \mid \text{RTA}_{r,\cdot,y-\delta})$ representing the likelihood that region $r$ develops specialization in technology $i$ at year $y$ given its past portfolio. These probabilities constitute **regional technological potential**—a forward-looking, region-specific measure of diversification feasibility.

Aggregating across technologies yields average regional potential:

$$p_{r,y} = \frac{\sum_{t} p_{r,t,y}}{N_T}$$

### Conceptual Interpretation

Technological potential differs fundamentally from relatedness density. While relatedness density assumes technologies combine linearly and symmetrically, potential:

1. Captures **non-linear interactions** between technologies through Random Forest's decision tree structure
2. Allows **asymmetric dependencies** where technology $t$ may enable $t'$ but not vice versa
3. Produces **region-specific estimates** rather than universal technology-pair relationships
4. Reflects **time-varying dynamics** by training on expanding windows of historical data

### Empirical Implications

High potential ($p_{r,t,y} \approx 1$) indicates a region's existing portfolio strongly predicts future specialization in technology $t$—the region likely possesses necessary complementary capabilities. Low potential ($p_{r,t,y} \approx 0$) suggests capability gaps despite potential relatedness. Crucially, potential varies across regions for the same technology, enabling analysis of how regional knowledge infrastructure, national ecosystems, and spatial factors moderate diversification feasibility—our core research questions.

## Feature Importance Technology Space (FITS)

Traditional technology networks use patent citations or co-occurrence patterns. Citations suffer from three limitations: (1) examiner-added citations may not reflect actual knowledge flows, (2) aggregating patent-level citations to technology-level relationships obscures directionality, and (3) citation networks are backward-looking rather than predictive [@fessina2024identifying]. Co-occurrence networks face the issues outlined in our problem statement: symmetry, noise, and linear assumptions.

FITS addresses these limitations by constructing asymmetric, predictive networks from machine learning. Rather than inferring relationships from co-occurrence, FITS extracts directional dependencies from the feature importance scores of our Technological Potential models.

### FITS Construction

Recall that for each technology $i$, we trained a Random Forest model predicting $z_{r,i,y}$ using $\{\text{RTA}_{r,t,y-\delta} : t \neq i\}$ as features. The feature importance $I_i(t)$ quantifies how much technology $t$ contributes to predicting future specialization in technology $i$ across all regions and time periods in the training data.

We formalize FITS as a directed, weighted network $G = (V, E, W)$ where:

- **Nodes** $V = \mathcal{T}$: the set of technologies
- **Edges** $E$: directed connections $(t \to i)$ for all $t, i \in \mathcal{T}, t \neq i$
- **Weights** $W_{t \to i} = I_i(t)$: feature importance of technology $t$ in model predicting technology $i$

We normalize weights within each target technology's model:

$$
W_{t \to i} = \frac{I_i(t)}{\sum_{t' \neq i} I_i(t')}
$$

ensuring that for each technology $i$, incoming edge weights sum to 1: $\sum_{t \neq i} W_{t \to i} = 1$.

### Mathematical Formulation (needs more work: simplify and clarify (construction + normalsiation))

From the Random Forest algorithm, feature importance for technology $t$ in model $M_i$ (predicting technology $i$) is:

$$
I_i(t) = \frac{1}{B}\sum_{b=1}^{B} \sum_{n \in T_b : v(n)=t} \Delta G(n)
$$

where the sum runs over all nodes $n$ in all trees $T_b$ that split on feature $t$, and $\Delta G(n)$ is the Gini impurity reduction at node $n$. Technologies that consistently create purer partitions when predicting $i$ receive higher importance scores.

The full FITS network aggregates these relationships across all 641 technologies, producing a $641 \times 641$ weighted adjacency matrix (excluding self-loops).

### Asymmetry and Hierarchy

Crucially, FITS is **asymmetric**: $W_{t \to i} \neq W_{i \to t}$ in general. This asymmetry captures hierarchical technological dependencies:

- If $W_{t \to i} \gg W_{i \to t}$, technology $t$ is a **prerequisite** or **stepping stone** toward $i$ (expertise in $t$ predicts future $i$, but not vice versa)
- If $W_{t \to i} \approx W_{i \to t}$, technologies are **complementary peers** (mutual predictive relationships)
- If $W_{t \to i} \ll W_{i \to t}$, technology $i$ is a prerequisite toward $t$

This hierarchical structure is invisible to symmetric co-occurrence measures and reveals technological trajectories: regions can identify which current capabilities enable paths toward desired future technologies.

### Conceptual Interpretation

FITS edges represent **predictive dependencies** based on historical diversification patterns across all European regions. A strong edge $t \to i$ indicates that regions with RTA in technology $t$ at time $y-\delta$ frequently developed RTA in technology $i$ by time $y$, even after controlling for all other technologies. This is fundamentally different from co-occurrence (which measures simultaneous presence) or citations (which measure backward-looking knowledge flows).

### Empirical Implications

FITS enables novel analyses of technological landscapes:

1. **Path identification**: For a target technology $i$, examine incoming edge weights to identify strong predictors (prerequisites)
2. **Branching points**: High out-degree nodes represent foundational technologies enabling diverse future specializations
3. **Category structure**: Aggregating edges by IPC categories reveals cross-domain dependencies (e.g., mechanical engineering → electronics)
4. **Regional positioning**: Comparing a region's current portfolio against FITS edge patterns reveals strategic opportunities and gaps

In our empirical analysis, FITS allows testing whether technology-specific characteristics (position in the network hierarchy, in/out-degree patterns, category embeddings) moderate diversification outcomes—aspects traditional relatedness measures cannot capture.


![Technology Network (based on 2018 patent data)](resources/technology_network.png)

The figure displays the FITS network with nodes colored by IPC category, sized by total degree, and transparency reflecting eigenvector centrality. Labels appear for high-centrality technologies. The structure reveals dense within-category connections and sparse but critical between-category bridges.

### Relatedness vs FITS: An example of Noise and Directionality


![](resources/h10l_eg.png)
![](resources/d06q_eg.png)

The relatedness method, built on symmetric co-occurrence patterns, captures technologies that frequently appear together in patent portfolios—but struggles to distinguish genuine relationships from statistical noise. For H01L (semiconductors), relatedness identifies some relevant connections like semiconductor memories (H10B) and packaging technologies (B68B), yet also surfaces spurious links: textile decoration (D06Q), woodworking (B27H), and fusion reactors (G21B). These appear not because they're technologically related, but because they happen to co-occur in diverse patent portfolios—a artifact of frequency-based aggregation that treats all co-presence equally.

FITS, in contrast, leverages asymmetric predictive relationships. Rather than asking "do these technologies appear together?", it asks "does expertise in technology A predict future development of technology B?" For semiconductors, this reveals H01L's role as an **enabling technology**: digital computing (G06F), telecommunications (H04L), and control systems (G05F) don't merely co-occur with semiconductors—they *depend* on them. The bidirectional structure exposes this hierarchy: strong edges flow FROM computing/telecom TO semiconductors, indicating these application domains require semiconductor capabilities as prerequisites, while weaker reverse edges show semiconductors don't particularly predict entry into computing. This asymmetry—invisible to symmetric relatedness—distinguishes foundational technologies from their applications.

The contrast sharpens with D06Q (textile decoration), where relatedness produces near-total noise: elevators (B64B), timing devices (G04D), and chemical production (C13B) appear simply because D06Q is rare and co-occurs randomly. FITS filters this noise effectively, identifying genuine textile domain technologies—sewing (D05B), textile treatment (D06M), wall coverings (D06N)—that share actual functional relationships. The method's ability to distinguish signal from statistical artifact, combined with its directional structure revealing technological hierarchies, demonstrates how FITS captures the **functional architecture** of innovation networks that co-occurrence frequencies obscure.



## Coherence: Bridging Technology Networks and Regional Portfolios

FITS identifies technology-to-technology relationships. Technological Potential quantifies regional diversification feasibility. **Coherence** bridges these levels by measuring the alignment between a technology's position in the FITS network and a region's existing specialization structure. In a way coherence can be viewed as sector level relatedness density.  


Consider two regions, both lacking specialization in technology $i$, both having similar potential $p_{r,i,y}$. However, Region A specializes in technologies that are strong predictors of $i$ (high incoming FITS edges), while Region B specializes in technologies unrelated to $i$ in the network. Coherence captures this difference: Region A has high coherence with $i$ (its portfolio aligns with $i$'s network prerequisites), while Region B has low coherence (misalignment).

This metric operationalizes the "knowledge coherence" and "cognitive proximity" concepts from innovation literature [@neffke2011how; @boschma2015towards] using our directional network structure. It enables testing whether diversification success depends not just on potential (predicted feasibility) but on the structural fit between regional portfolios and technology network positions.


For each region $r$, technology $i$, and IPC category $c$, we construct two embedding vectors capturing $i$'s directional network position and compare them to $r$'s average embeddings for technologies in category $c$.

**Technology embeddings** (individual technology $i$):

- Incoming: $\text{embcat\_to}_{i,c} = \frac{\sum_{t \in c} W_{t \to i}}{|\{t \in c : W_{t \to i} > 0\}|}$ (average FITS weight from category $c$ to technology $i$)
- Outgoing: $\text{embcat\_from}_{i,c} = \frac{\sum_{t' \in c} W_{i \to t'}}{|\{t' \in c : W_{i \to t'} > 0\}|}$ (average FITS weight from technology $i$ to category $c$)

**Regional average embeddings** (region $r$, category $c$):

- Incoming: $\overline{\text{embcat\_to}}_{r,c} = \frac{1}{|S_{r,c}|}\sum_{t \in S_{r,c}} \text{embcat\_to}_{t,c}$ where $S_{r,c} = \{t \in c : \text{RTA}_{r,t,y} \geq 1\}$
- Outgoing: $\overline{\text{embcat\_from}}_{r,c} = \frac{1}{|S_{r,c}|}\sum_{t \in S_{r,c}} \text{embcat\_from}_{t,c}$

**Coherence** is the cosine similarity between technology $i$'s directional embeddings and region $r$'s average directional embeddings for category $c$:

$$
\text{Coherence}_{r,i,c,y} = \frac{\mathbf{v}_1 \cdot \mathbf{v}_2}{||\mathbf{v}_1|| \cdot ||\mathbf{v}_2||}
$$

where:

- $\mathbf{v}_1 = [\text{embcat\_to}_{i,c}, \overline{\text{embcat\_to}}_{r,c}]$
- $\mathbf{v}_2 = [\text{embcat\_from}_{i,c}, \overline{\text{embcat\_from}}_{r,c}]$



Coherence ranges from -1 to 1:

- **High coherence** ($\approx 1$): Technology $i$'s FITS network position (both incoming and outgoing connections to category $c$) closely matches the average network position of technologies in which region $r$ specializes within category $c$. The region's existing capabilities align with the structural prerequisites and consequences of technology $i$.
- **Neutral coherence** ($\approx 0$): Misalignment between technology $i$'s relational structure and regional specialization patterns.
- **Negative coherence** ($\approx -1$): Technology $i$'s network position is opposite to the region's specialization structure (e.g., $i$ receives inputs from categories where the region sends outputs).

### Empirical Application

Coherence serves two roles in our empirical analysis:

1. **Interaction with Potential**: Test whether high potential translates to actual diversification only when coherence is also high (H2a: technology-specific characteristics moderated by regional knowledge coherence)

2. **Regional Infrastructure Measure**: Aggregate coherence across a region's non-specialized technologies indicates how well the regional portfolio is "positioned" in the FITS network for future diversification (captures knowledge infrastructure quality)

By incorporating coherence, we test whether successful diversification requires not just predicted feasibility (potential) and related capabilities (traditional relatedness), but also structural alignment between regional portfolios and network prerequisites—a form of contextualization that traditional measures cannot capture.


# Summary of Methodological Framework

Our approach replaces traditional relatedness constructs with machine learning-derived measures that enable contextualized diversification analysis:

- **Technological Potential** ($p_{r,t,y}$): Region-specific, non-linear, time-varying probabilities replace linear relatedness density
- **FITS Network** ($W_{t \to t'}$): Asymmetric, predictive dependencies replace symmetric co-occurrence-based relatedness  
- **Coherence** ($\text{Coherence}_{r,t,c,y}$): Structural alignment between regional portfolios and technology network positions captures knowledge infrastructure quality

Together, these measures allow testing how diversification is contingent on regional knowledge infrastructure (RQ2), national ecosystem characteristics (RQ3), and spatial factors (RQ4) in ways that traditional relatedness frameworks cannot—addressing the core problem of contextualizing diversification strategies beyond path dependency identification.
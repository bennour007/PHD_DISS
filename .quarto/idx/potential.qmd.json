{"title":"General Framework","markdown":{"headingText":"General Framework","containsRefs":false,"markdown":"\n# Random forest\n\n**Algorithmic Framework**\n\nRandom Forest constructs an ensemble of $B$ decision trees through bootstrap aggregating (bagging) with random feature subsampling. For binary classification where $y_i \\in \\{0,1\\}$, each tree $T_b$ is built on a bootstrap sample $\\mathcal{D}_b^*$ drawn with replacement from the original dataset.\n\n**Tree Construction via Recursive Partitioning**\n\nEach tree recursively partitions the feature space through binary splits. At node $t$, we randomly select $m$ features (typically $m = \\sqrt{p}$) and evaluate all possible splits within this subset. For feature $j$ and threshold $\\tau$, the split creates two child nodes: $t_L = \\{i : x_{ij} \\leq \\tau\\}$ and $t_R = \\{i : x_{ij} > \\tau\\}$.\n\n**Gini Impurity as Split Criterion**\n\nSplit quality is assessed via Gini impurity, defined as:\n\n$$G(t) = 1 - \\sum_{k=0}^{1} p_k^2(t) = 2p_0(t)p_1(t)$$\n\nwhere $p_k(t) = n_k(t)/n(t)$ represents the proportion of class $k$ observations at node $t$. Gini impurity quantifies node heterogeneity: $G=0$ indicates perfect purity (homogeneous class), while $G=0.5$ indicates maximum impurity (equal class distribution). The optimal split maximizes the weighted impurity reduction:\n\n$$\\Delta G(j, \\tau) = G(t) - \\left[\\frac{n(t_L)}{n(t)} G(t_L) + \\frac{n(t_R)}{n(t)} G(t_R)\\right]$$\n\nThe weighting by relative node size prevents trivial splits that isolate single observations into pure but uninformative leaves.\n\n**Termination and Class Assignment**\n\nRecursive splitting continues until predefined stopping criteria are met: node purity ($G=0$), minimum node size threshold, or maximum tree depth. Terminal nodes are assigned the majority class of their constituent observations.\n\n**Ensemble Prediction**\n\nFor prediction, observation $\\mathbf{x}$ traverses all $B$ trees. The final classification aggregates individual tree predictions via majority voting:\n\n$$\\hat{y}(\\mathbf{x}) = \\text{mode}\\{\\hat{y}_1(\\mathbf{x}), \\ldots, \\hat{y}_B(\\mathbf{x})\\}$$\n\nClass probabilities are estimated as the proportion of trees predicting each class: \n\n$$\\hat{P}(y=1|\\mathbf{x}) = B^{-1}\\sum_{b=1}^{B} \\mathbb{I}[\\hat{y}_b(\\mathbf{x})=1]$$\n\n**Interpreting Probability Estimates**\n\nThis probability represents the empirical vote share across trees. Values near 1 indicate strong consensus for class 1 (high confidence), while values near 0.5 reflect uncertainty with divided predictions. Unlike parametric models, these are purely data-driven vote proportions rather than model-based probability estimates. Practitioners can adjust classification thresholds based on asymmetric misclassification costs.\n\n**Variance Reduction Mechanism**\n\nThe algorithm's effectiveness stems from variance reduction through decorrelated predictions. Bootstrap sampling and random feature selection reduce inter-tree correlation $\\rho$, yielding ensemble variance:\n\n$$\\text{Var}(\\bar{y}) = \\rho\\sigma^2 + \\frac{1-\\rho}{B}\\sigma^2$$\n\nAs $B$ increases and $\\rho$ decreases, ensemble variance diminishes while maintaining the low bias of flexible tree models.\n\n**Feature Importance**\n\nFeature importance quantifies each predictor's contribution by aggregating Gini impurity reductions:\n\n$$I(j) = \\frac{1}{B}\\sum_{b=1}^{B} \\sum_{t \\in T_b : v(t)=j} \\Delta G(t)$$\n\nwhere the sum runs over all nodes using feature $j$ for splitting. Higher values indicate features consistently creating purer partitions. However, importance measures predictive association rather than causal effect, and suffer from bias toward high-cardinality features and correlated predictor sets. They serve to rank predictive relevance but require caution in causal interpretation.\n\n\n# Technological potential And Feature Importance Technology Space\n\nWhen two technologies (or products, activities, etc) share similar sets of input requirements (knowledge, resources, etc), we qualify them as related [@hausmann2011network]. This empirical observation has been confirmed in many areas in different streams of literature and formalised in the Principle of Relatedness(PoR) [@hidalgo2018principle]. However, deriving concrete policy implications from this principle has proven far from straightforward [@hidalgo2023policy; @li2024evaluating]. The PoR is essentially a framework that formalises a qualitative intuition that's been present in various streams of literature: The industrial fabric in a geographic location matters [@hidalgo2021economic; @hidalgo2009building]. This framework enables researchers to derive different metrics that quantify path dependency and, therefore, infer more granular and pragmatic policy recommendations [@li2024evaluating]. However, we often find in the literature that many studies refrain from investigating beyond the identification of path dependencies[@hidalgo2023policy]. Although such identification may prove interesting at times, the entire idea of the PoR is to be used to break free from the path dependency curse and focus on unique regional paths that promote diversification [@imbs2003stages]. Diversification is the endpoint because it creates different sets of non-fungible tacit and non-tacit knowledge/capacity [@collins1974tea] that can be compounded over time and across industries to create value that drives regional growth and development [@dosi1982technological; @weitzman1998recombinant]. Knowledge, in all its forms, is the driver of the PoR policy implications [@balland2022scientific; @balland2019smart]. And although identifying promising areas of knowledge(related or unrelated) is a useful exercise. Figuring out the unique elements that dictate the dynamics of knowledge flows is at the heart of the industrial policies in this context [@nomaler2024related]. Moreover, the PoR is also complemented by the Economic Complexity paradigm(EC) pioneered for the first time in [@hidalgo2009building]. EC is a methodological framework that builds on the PoR and frames economies as complex systems. The idea is simple: an economy, regardless of its scale, is a complex system that might be impossible to determine the entirety of its components. But if we quantify the interactions between different systems and their different components, then we can estimate indices and metrics that capture most of the variation. In this sense, the PoR quantifies path dependency patterns(via metrics like relatedness and proximity), and EC quantifies the sophistication of specialisation patterns(via metrics like complexity and fitness)[^1]. One can also describe relatedness as a variation of a recommendation system and complexity as a dimensionality reduction exercise. However, there's still no general consensus on the reliability of any given methodology for both exercises, regardless of the popularity of one or the other. What there's a consensus on, however, is that these frameworks and the toolbox they provide can be improved further as stated in @pinheiro2025. Regardless, EC and PoR were adopted in many policy papers such as @zaccaria2018integrating, @albora2021economic and @albora2025economic.\n\n[^1]: We will use the terms relatedness and complexity to refer to these two dimensions moving forward following the literature nomenclature\n\nIn this context, the literature provides more threads of ideas that target a deeper understanding and analysis of complex economic systems. For instance, investigations and studies regarding unrelated diversification [@pinheiro2022time; @boschma2023role], geographic inequalities [@pinheiro2025dark; @hartmann2017linking], emerging industries/technologies [@lee2018early; @fessina2024identifying], and diversification strategies [@alshamsi2018optimal], among others, are pioneering the effort to bridge different gaps in theory, policy implications, and methodology. These ideas, among others, suggest that investing in unrelated activities can yield greater value and help break free from the path‐dependency curse—a phenomenon the literature shows exacerbates regional inequalities. Thus, one of the challenges is to quantify how to expand related activities beyond path dependency(strategy), which activity/sector to aim for(target), and the requirements for such an investment to be fruitful(condition). The scope of our study is expanding ideas around diversification strategies and conditions.\n\nPolicy makers often face a difficult choice when deciding on industrial upgrading. The first is to take advantage of existing local capacity and knowledge(related diversification). This choice is presumably the easiest one, since the local economy already has what it takes for the implementation [@boschma2017relatedness]. The second is to invest in building new capacity/knowledge(unrelated diversification) with all the risks that such a gamble accommodates [@coniglio2021evolution]. These choices have been at the centre of different theories in development economics(big push, forward/backward linkages [@rosenstein1943problems; @hirschman1958strategy], etc). However, we argue on the side of [@pinheiro2025] that the basis of this narrative is incomplete since it contains an implicit assumption that is often overlooked: it's easier for an economic system to diversify into a related activity than an unrelated one. But how do we assess the ease of diversification, accounting for its level of relatedness?\n\n\n\n\n@lee2017catch show that for every stage of industrial maturity different level of diversity among other \"initial-conditions\" is needed. The main idea is that local capacities are necessary but insufficient condition for related diversification. This has been formalised explicitly in [@hausmann2011network]. where the authors define the \"quiescence trap\", which can be observed when a country with few capabilities face low incentive to accumulate new ones. Additionally, in a broader policy context, and even if we assume that these initial conditions are met locally, and related diversification is feasible, it may simply exacerbate the regional disadvantages. Indeed related diversification has been observed to increase the gaps between locations [@mealy2022them; @pinheiro2025dark]. Essentially, if we have two locations, one already has a range of complex capacities, the other doesn’t. If policy only backs what each location already does well, the complex location keeps getting ever more complex, drawing more investment and talent, while the other one falls further behind exacerbating inequality. Moreover, there's no consensus on one way to quantify relatedness. The literature usually relies on the co-occurrence matrix to construct the relatedness between products/technologies etc. [@coniglio2021evolution] point out that most studies in this context do not differentiate between random co-occurrences, and co-occurrences that are due to related capacities and proposes a test to investigate these significance of these relationships. Similarly, the proposed methodology in [@albora2023product] responds to the same criticism and argue that the number of products/technologies almost always outnumber that of regions/locations, therefore the information extracted from the co-occurrence matrix is at best a random walk.\n\n## Technological Potential\n\nWe follow the methodology proposed by [@albora2023product] for trade data. The methodology named product progression, is based on a machine learning approach that enables researchers to unravel novel aspects of their RCA data. In our case that would be the non-linear dependence between technologies. Our Objective from this phase is to eventually predict whether a region will develop an expertise in a given technology. For reasons of data availability in other data bases that we will use in the next stages, we opted to limit these predictions to an 11 years interval spanning from 2008 to 2018 using data from 4 years ago for each prediction. These predicted probabilities are precisely the regional technological potential. They denote a hypothetical situation that describe for each region, the technologies it has potential to develop expertise in given the relationships that we already modeled.\n\n\nWe use Data from the European Patent Office, which contains details on patent applications from 1978 to 2021. The EPO provides a rigorous and detailed classification of each patent application up to 8 or more digits. In our case, we consider the IPC classifications, but since these classifications are extremely granular and are considerably larger than the regions observed (at 8000+ classes), we limit our data to the 4th digit of the IPC classification. These 4 digits contain 3 layers of information with which we can define a given technology, a section denoted by a letter, a class denoted by two digits, and a subclass denoted by another letter. Thus, an IPC class/technology such as F16H is structured hierarchically: Section F covers Mechanical engineering (including lighting, heating, weapons, and blasting), Class 16 pertains to engineering elements and general methods for producing and transmitting mechanical power, and Subclass H specifically addresses gears, shaft connections, and gearing for conveying rotary motion. With such a subset, we ended up with 641 distinct technologies. Additionally, the same data also provides details on where the applications were made, we capture these details at the NUTS2 level[^2] for 34 European countries within and outside the European Union, spanning across 345 regions. Additionally we also use data from the Eurostat database to incorporate regional level socio-economic factors which are detailed further in section @sec-factors and summarised in @tbl-sum.\n\n[^2]: With the exception of Belgium and the United Kingdom who were included at NUTS1 level\n\nFurthermore, we quantify regions' specialisation by means of the Revealed Comparative Advantage(RCA) [@balassa1965rca]. In our context, the RCA measures the region's relative specialization level in a given technology, which enables us to capture both expertise and diversity when we aggregate all the technologies for each region. This measure, also known as the Balassa index, proved useful in determining complex and non-linear relationships between products/activities. Although the RCA is mainly designed for use with international trade data, it has also been adopted in the literature on the geography of innovation. Simply put, the RCA quantifies simultaneously the relative level and the quality of co-occurrence, which reduces the noise in the data. Although some papers criticise the use of the RCA with patent classes [@balland2019beyond; @diodato2023economic], we think it fits our objective in capturing meaningful relationships between technologies. We compute these RCA measures to obtain, for each year, a matrix denoting the regions in its rows and the technologies in its columns. We formalize it as follows: Let $X_{r,t,y}$ be the measure of activity (patent counts) of region $r$ in technology $t$ during year $y$. Where $\\mathcal{T}$ is the set of technologies, $\\mathcal{Y}$ is the set of years, $\\mathcal{R}$ is the set of regions, and $\\mathcal{C}$ is the set of countries, such that:\n\n$$\n\\mathcal{T} = \\{\\,t : 1 \\le t \\le N_T\\},\\quad\n\\mathcal{Y} = \\{\\,y : 1 \\le y \\le N_Y\\},\\quad\n\\mathcal{R} = \\{\\,r : 1 \\le r \\le N_R\\},\\quad\n\\mathcal{C} = \\{\\,c : 1 \\le c \\le N_C\\}.\n$$\n\nAnd $N_T, N_Y, N_R, N_C$ are the total counts of technologies, years, regions and countries.\n\nThe RCA of region $r$ in technology $t$ in year $y$ is\n\n$$\n\\mathrm{RCA}_{r,t,y}\n= \\frac{\\displaystyle\\frac{X_{r,t,y}}{\\sum_{t'} X_{r,t',y}}}\n       {\\displaystyle\\frac{\\sum_{r'} X_{r',t,y}}{\\sum_{r',t'} X_{r',t',y}}}\n= \\frac{X_{r,t,y}\\,\\sum_{r',t'}X_{r',t',y}}\n       {\\bigl(\\sum_{t'}X_{r,t',y}\\bigr)\\,\\bigl(\\sum_{r'}X_{r',t,y}\\bigr)}\n$$\n\nFor each year $y$, we then assemble the **RCA matrix** $\\mathbf{R}^{(y)}$ whose $(r,t)$-entry is $\\mathrm{RCA}_{r,t,y}$:\n\n$$\n\\mathbf{R}^{(y)}\n= \\bigl[\\mathrm{RCA}_{r,t,y}\\bigr]_{r=1,\\dots,N_{R}}^{t=1,\\dots,N_{T}}\n$$\n\nThese yearly measures are essential for us, since our entire approach depends on different manipulations around these stacked matrices.\n\n\n\n\nThe modeling of the Random Forest algorithm is not intuitive in this proposed methodology. In fact the novelty of the approach proposed by [@albora2023product] is not the use of a tree-based algorithm, but rather to model each technology separately. The idea is to construct a model for every technology in $\\mathbf{R}^{(y)}$ matrix such that the target technology $i, i\\in \\mathcal{T}$ is the outcome and the features are all other technologies different than $i$. The trick here is to binarise the outcome and leave the features as they are for every model we train such that:\n\n$$\nz_{r,t,y}\n\\;=\\;\n\\begin{cases}\n1 & \\text{if }\\mathrm{RCA}_{r,t,y}\\ge1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nThe $z_{r,t,y}$ term reflect the capacity of region $r$ at year $y$ for technology $t$. In here capacity means that a region has an advantage/specialised in that specific technology relative to the other regions. Additionally we include a 4 year lag, or a fixed horizon we call delta, $\\delta=4$ in the features since the entire idea is to assess the capacity of the current outcomes based on the past features. This aligns with instincts in the literature in which studies like [@andreoni2019political] posit that past capacities predict future diversification. Eventually the predicted outcomes describe what technology is possible to develop expertise in, given the observed capacity $\\delta$ years ago. However, as stated in @albora2023product choosing the value of $\\delta$ is challenging since increasing it decrease the performance of the models. Our choice here, relies on this observation and is the most optimal decision since we train the models for different years instead of just one, thus we need to have for each model at each year of prediction enough observations.\n\nThe training and testing sets are constructed consequently:\n\nWe use a fixed horizon ($\\delta=4$) years to predict future expertise. Let years run from $y = y_0...y_f$, where $y_0 = 1978$ and $y_f = 2018$, let's also consider the target year of prediction $y_t \\in \\{2008,..., 2018\\}$. We then have:\n\n$$\nX_{\\text{train}} = \\{ \\mathrm{RCA}_{r,t,y} | y \\in [y_0, y_t - 2\\delta]\\},\\quad Y_{\\text{train}} = {z}_{r,t',y} | y \\in [y_0 + \\delta,  y_t - \\delta] \\}\n$$\n\n$$\nX_{\\text{test}} =  \\{ \\mathrm{RCA}_{r,t,y} |y_t - \\delta\\},\\quad \nY_{\\text{test}} = \\{ z_{r,t,y} |y_t\\}\n$$ Given the complexity of computations, which would require infeasible timing, we conducted cross validation on a random sample of models targeting G06G (Analog computers for data processing), B67B (Closing bottles, jars, or similar containers), D02J (Mechanical finishing or refining of yarns), and C08J (Working-up plastics-processing, recovery, or treatment of waste). Then we chose the parameter values with the most frequency and used them for the rest of the models specifically: mtry = 139, trees = 100, min_n = 38. For our case and computational constraints, this was the only feasible approach. The training was conducted in R using the Ranger package [@wright2017ranger], with targets [@landau2021targets] as a pipeline orchestrator and the tidymodels framework.\n\nOnce we train our models one for each technology for each of the 11 years (7051 models in total) we obtain at each year, and for each technology, the probability that a region develops expertise. We define this set of probabilities as $P(\\mathrm{RCA}_{r,t,y} \\geq 1)$ which we write simply as $p_{r,t,y}$ and we will refer to these probabilities as the regional technological potential. When we aggregate these probabilities regionally, we obtain the (average) regional potential $p_{r,y} = \\frac{\\sum_{t} p_{r,y,t}}{n_t}$, with $n_t$ the corresponding number of observations.\n\n## Feature Importance Technology Space\n\n\nA natural approach to constructing technology networks would be to use patent citation data: if Patent A cites Patent B, and they contain different technologies, this reveals a relationship between those technology domains. However, this approach presents three critical limitations for our purposes. First, EPO citations include examiner-added citations that may not reflect actual knowledge flows between inventors. Second, aggregating patent-level citations to technology-level relationships relies on co-occurrence patterns that obscure the directionality of knowledge dependencies—whether technology A enables B or vice versa. Third, citation networks are inherently backward-looking, reflecting past relationships rather than predicting future technological trajectories.\n\nTo address these limitations, we adopt the Feature Importance Product Space (FIPS) methodology [@fessina2024identifying]. Rather than inferring relationships from citation co-occurrence, FIPS uses machine learning to identify which existing technology specializations predict future specialization in other technologies. Specifically, we train random forest models where the current presence of regional expertise in technology T is predicted by past expertise patterns across all other technologies. The resulting feature importance scores reveal directional, predictive relationships: if expertise in technology A strongly predicts future expertise in B, this indicates A is a \"stepping stone\" toward B, even if the reverse is not true. This asymmetric structure captures hierarchical technological dependencies that symmetric co-occurrence measures would miss, and its forward-looking nature aligns with our focus on how current network positions influence future patent value.\n\n\n![Technology Network(based on 2018 patent data)](resources/technology_network.png)\n\n\nFollowing [@fessina2024identifying], we quantify specialization patterns by means of the Revealed Comparative Advantage(RCA) [@balassa1965rca]. Although the RCA is mainly designed for use with international trade data, it has also been adopted in the literature of the geography of innovation following the Balland nomenclature [@balland2017geography] we will refer to this metric as the Revealed Technological Advantage(RTA) instead. The RTA measures a location's relative specialization level in a given technology which enables us to capture both expertise and diversity when we aggregate all the technologies for each location. This measure, proved useful in determining complex and non-linear relationships between products/activities. Simply put, the RTA quantifies simultaneously the relative level and the quality of co-occurrence, which reduces the noise in the network data. Although some papers criticise the use of the RCA/RTA with patents classes [pinheiro2025], we think it fits our objective in capturing meaningful relationships between technologies when we use them in the framework of [@fessina2024identifying]. We compute these measures to obtain for each year a matrix denoting the regions in its rows and the technologies in its columns. We formalize it as follows: Let $X_{r,t,y}$ be the measure of activity (patent counts) of region $r$ in technology $t$ during year $y$. Where $\\mathcal{T}$ is the set of technologies, $\\mathcal{Y}$ is the set of years, and $\\mathcal{R}$ is the set of regions.\n\n\n\n\n# Diversification, coherence, and the role of space\n\nCoherence measures the alignment between a technology's directional embedding in the network (incoming vs. outgoing edge strengths to different categories) and the regional average embedding pattern for specialized technologies in that category. \n\nWe construct this metric using cosine similarity which ranges from -1 to 1. Values near 1 indicate (high coherence) indicate the technology's network position matches the region's technological profile; values near 0 (neutral coherence) suggest misalignment between the technology's relational structure and regional specialization patterns; whereas values near -1 suggest a complete mismatch.\n\nFor each region-technology-category-year combination, defined as :\n\n$\\mathcal{T}$ is the set of technologies, $\\mathcal{Y}$ is the set of years, $\\mathcal{R}$ is the set of regions, and $\\mathcal{C}$ is the set of categories, such that:\n\n$$\n\\mathcal{T} = \\{\\,t : 1 \\le t \\le N_T\\},\\quad\n\\mathcal{Y} = \\{\\,y : 1 \\le y \\le N_Y\\},\\quad\n\\mathcal{R} = \\{\\,r : 1 \\le r \\le N_R\\},\\quad\n\\mathcal{C} = \\{\\,c : 1 \\le c \\le N_C\\}.\n$$\n\nAnd $N_T, N_Y, N_R, N_C$ are the total counts of technologies, years, regions and categories\n\nwe measure the following cosine similarity:\n\n$$\\text{Coherence}_{r,i,t} = \\frac{\\mathbf{v}_1 \\cdot \\mathbf{v}_2}{||\\mathbf{v}_1|| \\cdot ||\\mathbf{v}_2||}$$\n\nwhere:\n\n\n- $\\mathbf{v}_1 = [\\text{embcat\\_to}_{i}, \\overline{\\text{embcat\\_to}}_{r,c}]$ captures incoming edge patterns\n- $\\mathbf{v}_2 = [\\text{embcat\\_from}_{i}, \\overline{\\text{embcat\\_from}}_{r,c}]$ captures outgoing edge patterns\n\n\n- $\\text{embcat\\_to}_{i} = \\frac{\\sum \\text{weights\\_to}_{i,c}}{\\text{count(incoming edges)}_{i,c}}$ measures average incoming edge weight from category $c$ to technology $i$\n- $\\text{embcat\\_from}_{i} = \\frac{\\sum \\text{weights\\_from}_{i,c}}{\\text{count(outgoing edges)}_{i,c}}$ measures average outgoing edge weight from technology $i$ to category $c$\n- weights = Normalised Feature Importance values denoting connection strength between technology pairs. \n\n\n- $\\overline{\\text{embcat\\_to}}_{r,c}$ = regional average of incoming embeddesdness for technologies with RCA≥1 in category $c$ within region $r$\n- $\\overline{\\text{embcat\\_from}}_{r,c}$ = regional average of outgoing embeddesdness for technologies with RCA≥1 in category $c$ within region $r$\n\n\n\n\n\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"potential.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","bibliography":["references.bib"],"browser":"brave","theme":"cosmo","lot":true,"lof":true,"fontsize":"12pt","linestretch":1.5,"mermaid":{"theme":"default"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
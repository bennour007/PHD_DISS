## In here we clean the data
## we add network measurments
## we test the hiarchichal model 


pacman::p_load(patchwork, ggraph, tidygraph, networkD3, lsa, igraph, mgcv,
               tidyverse, targets, gt, ggiraph, lme4, brms, glmmTMB)

# tmp <- tar_read(path_results, store = here::here("_targets_network")) %>% bind_rows()
selected_countries <- c(
  "AT", "BE", "BG", "CH", "CY", "CZ", "DE", "DK", "EE", "EL", "ES", "FI", "FR", 
  "HR", "HU", "IE", "IT", "LT", "LU", "LV", "MT", "NL", "NO", "PL", "PT", "RO", 
  "SE", "SI", "SK", "UK"
)

gei_2018 <- readxl::read_xlsx(here::here("data", "GEI-2018-data.xlsx")) %>% 
  janitor::clean_names() %>% 
  mutate(year  = 2018)
gei_06_16 <- readxl::read_xlsx(here::here("data", "GEI_dataset_2006_2016.xlsx")) %>% 
  janitor::clean_names() %>% 
  filter(year %in% c(2008:2016)) %>% 
  rename(institutional = institutional_scores, individual = individual_scores)

probs <- read_csv(here::here("data", "prob_data_thresh.csv")) %>% 
  ### THIS IS PROBLEMATIC, SPECIFICALLY: target == 0
  mutate(potential = if_else(target == 0 & pred1 >= threshold, 1, 0)) %>% 
  filter(!str_starts(region, "JE|IM")) %>% 
  ungroup() %>% 
  mutate(
    region2 = case_when(
      # PORTUGAL MAPPING (old → new)
      region == "PT19" ~ "PT18",
      region == "PT1A" ~ "PT17", 
      region == "PT1B" ~ "PT18",
      region == "PT1C" ~ "PT18",
      region == "PT1D" ~ "PT18",
      
      # NETHERLANDS MAPPING (old → new)
      region == "NL35" ~ "NL33",
      region == "NL36" ~ "NL31",
      
      # UK MAPPING (NUTS 2013 → NUTS 2016) - VERIFIED
      # North East England
      region == "UKC3" ~ "UKC1",
      region == "UKC4" ~ "UKC2",
      
      # Eastern England  
      region == "UKH2" ~ "UKH1",
      region == "UKH3" ~ "UKH2",
      region == "UKH4" ~ "UKH3",
      region == "UKH5" ~ "UKH1",  # VERIFIED
      region == "UKH6" ~ "UKH1",  # VERIFIED
      
      # South West England
      region == "UKK3" ~ "UKK1",
      region == "UKK4" ~ "UKK2", 
      region == "UKK5" ~ "UKK3",
      region == "UKK6" ~ "UKK4",
      region == "UKK7" ~ "UKK1",  # VERIFIED
      
      # Wales
      region == "UKL3" ~ "UKL1",
      region == "UKL4" ~ "UKL2",
      region == "UKL5" ~ "UKL2",  # VERIFIED
      
      # Scotland
      region == "UKM0" ~ "UKM7",
      region == "UKM1" ~ "UKM5",
      region == "UKM2" ~ "UKM7",  
      region == "UKM3" ~ "UKM9",
      
      # Keep all other regions unchanged
      TRUE ~ region
    )
  )

names <- gei_06_16 %>%  colnames() %>% .[-1]

GEI_data <- gei_2018 %>% 
  select(all_of(names)) %>% 
  bind_rows(gei_06_16 %>% select(all_of(names))) %>% 
  mutate(nuts0 = countrycode::countrycode(country, 
                             origin = "country.name", 
                             destination = "iso2c")) %>% 
  select(country, nuts0, everything()) %>% 
  filter(nuts0 %in% selected_countries)

main_g <- tar_read(main_g, store = here::here("_targets_network"))

main_g <- main_g %>% set_names(c(2008:2018))
raw_data <- tar_read(ind_prep, store = here::here("_targets_ML")) 
# ffdata <- tar_read(fulfilled_data, store = here::here("_targets_ML"))

# paths_data <- tar_read(path_results, store = here::here("_targets_network")) %>% 
#   bind_rows() %>% 
#   group_by(origin, destination) %>%
#   pivot_longer(
#     cols = c(path_in, path_out, dist_in, dist_out),
#     names_to = c(".value", "direction"),
#     names_pattern = "(.+)_(in|out)"
#   ) %>%
#   mutate(
#     w_dist_inv = map_dbl(
#       dist,
#       ~ .x %>% pluck("weighted_distance")
#     ),
#     raw_dist = map_dbl(
#       dist,
#       ~ .x %>% pluck("raw_distance")
#     ),
#     node_path = map_chr(
#       path,
#       ~ paste(names(.x$vpath[[1]]), collapse = ",")
#     ),
#     edge_path = map_chr(
#       path,
#       ~ paste(.x$epath[[1]], collapse = ",")
#     ),
#     hops = str_count(node_path, ","),
#     across(
#       w_dist_inv:edge_path,
#       ~ if_else(is.infinite(.x) | .x == "", NA, .x)
#     )
#     # node_path = if_else(node_path == "", NA, node_path),
#     # edge_path = if_else(edge_path == "", NA, edge_path)
#   )
#   # dplyr::select(-path, -dist, od_id = tar_group)
# 
# paths_data %>% 
#   select(-path, -dist) %>% 
#   write_csv(here::here("data", "paths_data.csv"))

# paths_data <- read_csv(here::here("data", "paths_data.csv"))

# potential_nodes <- paths_data %>%
#   pull(destination) %>%
#   unique()
# 
# regional_nodes <- paths_data %>%
#   pull(origin) %>%
#   unique()


raw_data_long <- raw_data %>% 
  filter(app_year %in% c(2008:2018)) %>% 
  mutate(
    rca = map(indicators, ~ pluck(.x, "rca")),
    rca_long = map(rca, ~ .x %>% rename(reg = region) %>%  pivot_longer(2:last_col(), names_to = "tech", values_to = "rca"))
  ) %>% 
  select(-indicators, -rca) %>% 
  unnest(c(data, rca_long)) %>% 
  select(-industry, -reg, -tar_group, patent_n = value) %>% 
  ungroup() %>% 
  filter(!str_starts(region, "JE|IM")) %>% 
  ungroup() %>% 
  mutate(
    region2 = case_when(
      # PORTUGAL MAPPING (old → new)
      region == "PT19" ~ "PT18",
      region == "PT1A" ~ "PT17", 
      region == "PT1B" ~ "PT18",
      region == "PT1C" ~ "PT18",
      region == "PT1D" ~ "PT18",
      
      # NETHERLANDS MAPPING (old → new)
      region == "NL35" ~ "NL33",
      region == "NL36" ~ "NL31",
      
      # UK MAPPING (NUTS 2013 → NUTS 2016) - VERIFIED
      # North East England
      region == "UKC3" ~ "UKC1",
      region == "UKC4" ~ "UKC2",
      
      # Eastern England  
      region == "UKH2" ~ "UKH1",
      region == "UKH3" ~ "UKH2",
      region == "UKH4" ~ "UKH3",
      region == "UKH5" ~ "UKH1",  # VERIFIED
      region == "UKH6" ~ "UKH1",  # VERIFIED
      
      # South West England
      region == "UKK3" ~ "UKK1",
      region == "UKK4" ~ "UKK2", 
      region == "UKK5" ~ "UKK3",
      region == "UKK6" ~ "UKK4",
      region == "UKK7" ~ "UKK1",  # VERIFIED
      
      # Wales
      region == "UKL3" ~ "UKL1",
      region == "UKL4" ~ "UKL2",
      region == "UKL5" ~ "UKL2",  # VERIFIED
      
      # Scotland
      region == "UKM0" ~ "UKM7",
      region == "UKM1" ~ "UKM5",
      region == "UKM2" ~ "UKM7",  
      region == "UKM3" ~ "UKM9",
      
      # Keep all other regions unchanged
      TRUE ~ region
    )
  )
  

# 
# paths_data_minimal <- paths_data %>%
#   group_by(origin, destination, year, region) %>%
#   slice_min(w_dist_inv, with_ties = T, na_rm = T) %>%
#   ungroup() %>%
#   group_by(destination, year, region) %>%
#   slice_min(w_dist_inv, with_ties = T, na_rm = T)
# 
# rm(paths_data)
gc()

tg <- main_g %>% 
  set_names(c(2008:2018)) %>%  # Set names to c(2014, 2016, 2018)
  imap(
    ~ .x %>% as_tbl_graph()
  )


# Step 1: Extract nodes with categories
nodes_with_cat <- tg %>%
  imap(
    ~ .x %>% 
      activate(nodes) %>%  
      as_tibble() %>% 
      mutate(
        year = .y, 
        category = str_sub(name, end = 1)
      )
  ) %>% 
  bind_rows()

# Step 2: Match by index
edges_with_cat <- tg %>%
  imap(
    ~ .x %>% 
      activate(edges) %>%  
      as_tibble() %>% 
      mutate(
        year = .y
        # from_cat = str_sub(from, end = 1),
        # to_cat = str_sub(to, end = 1)
      )
  ) %>% 
  bind_rows() %>% 
  mutate(
    from_name = nodes_with_cat %>% pull(name) %>% .[from],
    to_name = nodes_with_cat %>% pull(name) %>% .[to],
    from_cat = nodes_with_cat %>% pull(category) %>% .[from],
    to_cat = nodes_with_cat %>% pull(category) %>% .[to]
  ) 

## tech is reference, to the tech, from the tech
to_node <- edges_with_cat %>% 
  group_by(year, name = to_name, cat = from_cat) %>% 
  summarise(
    weights_to = sum(weight),
    oc_to = n()
  ) %>% 
  ungroup() 
  

from_node <- edges_with_cat %>% 
  group_by(year, name = from_name, cat = to_cat) %>% 
  summarise(
    weights_from = sum(weight),
    oc_from = n()
  ) %>% 
  ungroup() 

nodes_emb <- left_join(to_node, from_node) %>% 
  mutate(
    across(everything(), ~ replace_na(.x, 0)),
  ) %>% 
  group_by(year, name) %>%
  mutate(
    total_from = sum(oc_from),
    totW_from = sum(weights_from),
    pct_from = oc_from / total_from * 100,
    pctW_from = weights_from / totW_from,
    total_to = sum(oc_to),
    totW_to = sum(weights_to),
    pct_to = oc_to / total_to * 100,
    pctW_to = weights_to / totW_to
  ) %>%
  ungroup()
 

nodes_emb_wide <- nodes_emb %>% 
  ungroup() %>% 
  pivot_longer(weights_to:pctW_to, names_to = "metric") %>% 
  separate(metric, into = c("metric", "direction"), sep = "_") %>% 
  pivot_wider(
    names_from = c(cat, direction), values_from = "value"
  ) %>% 
  mutate(
    across(everything(), ~ replace_na(.x, 0))
  )


nodes_emb_long <- nodes_emb %>% 
  ungroup() %>% 
  pivot_longer(weights_to:pctW_to, names_to = "metric") %>% 
  separate(metric, into = c("metric", "direction"), sep = "_") %>% 
  # pivot_wider(
  #   names_from = c(cat, direction), values_from = "value"
  # ) %>% 
  mutate(
    across(everything(), ~ replace_na(.x, 0))
  )


node_emb_own <- nodes_emb %>% 
  filter(str_sub(name, end = 1) == cat)


unique_cats <- nodes_emb_long %>% 
  pull(cat) %>% 
  unique()


nodes_strength <- nodes_emb %>%
  ungroup() %>% 
  mutate(
    year = as.numeric(year),
    embcat_to = weights_to/oc_to,
    embcat_from = weights_from/oc_from,
    across(
      embcat_to:embcat_from,
      ~ ifelse(is.nan(.x) | is.infinite(.x), 0, .x)
    )
  ) %>% 
  select(app_year = year, tech = name, cat, embcat_to, embcat_from, oc_to, oc_from, weights_to, weights_from)

nodes_strength %>%  count(tech)

coherence <- raw_data_long %>% 
  left_join(
    nodes_strength,
    by = c("app_year", "tech")
  ) %>% 
  # filter(rca >= 1) %>% 
  group_by(app_year, region2, cat) %>% 
  mutate(
    r_embcat_to = mean(embcat_to[rca >= 1], na.rm = T),
    r_embcat_from = mean(embcat_from[rca >= 1], na.rm = T)
  ) %>%
  ungroup() %>% 
  mutate(
    across(
      where(is.numeric),
      ~ replace_na(.x, 0)
    )
  ) %>% 
  group_by(app_year, region2, tech) %>%
  summarise(
    coherence = {
      v1 <- c(embcat_to, r_embcat_to)
      v2 <- c(embcat_from, r_embcat_from)
      if(sum(v1^2) == 0 | sum(v2^2) == 0) 0 else as.numeric(cosine(v1, v2))
    }
    # coherence = cosine(c(embcat_to, r_embcat_to), c(embcat_from, r_embcat_from)) %>%  as.numeric()
  ) %>%
  ungroup() 
  # group_by(app_year, region) %>%
  # mutate(
  #   r_coherence = mean(rt_coherence, na.rm = T),  # or weighted mean
  #   r_alignment = sd(rt_coherence, na.rm = T)     # consistency across categories
  # ) %>%
  # pull(coherence) %>% 
  # hist()


### complementarity


main_g %>% 
  map(~ as_adjacency_matrix(.x, attr = "weight", sparse = F)) -> tmp

nested_mats <- tibble(
  name = names(tmp) %>% as.numeric(),
  matrix = tmp
) %>% 
  mutate(
    tech = map(matrix, ~ rownames(.x)),
    matrix = map(matrix, ~ {
      row_sums <- rowSums(.x)
      row_sums[row_sums == 0] <- 1  # Avoid division by zero
      .x / row_sums
    })
  )


nested_coherence <- coherence %>% 
  group_by(app_year) %>% 
  nest() %>% 
  mutate(tech_r = map(data, ~ .x %>% pull(tech) %>% unique()))


matrix_coherence <- left_join(
  nested_coherence, 
  nested_mats,
  by = c("app_year" = "name")
) %>% 
  mutate(
    reg_dat = map2(
      .x = data,
      .y = tech,
      \(.x,.y) .x %>% filter(tech %in% .y)
    )
  ) 

RRC <- matrix_coherence %>% 
  mutate(
    # Create P matrix (regions × techs) from reg_dat
    P = map2(reg_dat, tech, function(df, tech_order) {
      df %>%
        pivot_wider(
          id_cols = region2,
          names_from = tech, 
          values_from = coherence,
          values_fill = 0
        ) %>%
        column_to_rownames("region2") %>%
        select(all_of(tech_order)) %>%  # Match W column order
        as.matrix()
    }),
    
    # Compute C = P %*% W %*% t(P)
    C = map2(P, matrix, ~ .x %*% .y %*% t(.x))
  )

# RRC[3,8] %>%   unnest()
#   select(reg_dat) %>% 
#   ungroup() %>%   
#   filter(app_year == 2014) %>% 
#   unnest() %>% 
#   summary()


cat_emb <- edges_with_cat %>% 
  group_by(from_cat, to_cat, year) %>% 
  summarise(
    oc = n(),  
    avg_weight = mean(weight, na.rm = TRUE)
  ) %>% 
  ungroup() %>% 
  group_by(year) %>% 
  mutate(
    total = sum(oc),
    pct = oc/total
  )




  
regional_data <- probs %>% 
  # filter(
  #   app_year_o %in% c(2014, 2016, 2018)
  # ) %>%
  rename(year = app_year_o, rca_bin = target, tech = ref_tech) %>% 
  left_join(
    raw_data_long,
    by = c("year" = "app_year", "tech", "region2")
  ) %>% 
  left_join(
    coherence,
    by = c("year" = "app_year", "tech", "region2")
  ) %>% 
  mutate(cat = str_sub(tech, end = 1)) 
  # left_join(
  #   # node_emb_own %>% 
  #   nodes_emb_long %>% 
  #     filter(metric == "oc") %>%
  #     mutate(year = as.numeric(year)),
  #   by = c("year", "tech" = "name", "cat")
  # )

## calculate distance betwenn regional nodes and potential nodes 


## all regional nodes for each year vector

origin_list <- regional_data %>%
  filter(rca >= 1) %>%
  group_by(year) %>%
  summarise(origins = list(unique(tech))) %>%
  deframe()  # Named list: year -> vector of techs

destination_list <- regional_data %>%
  filter(potential == 1 ) %>%
  group_by(year) %>%
  summarise(destinations = list(unique(tech))) %>%
  deframe()  # Named list: year -> vector of techs
  

years <- names(origin_list)  # Or unique(data_reg$year)


distances_by_year <- map_dfr(years, function(yr) {
  
  # Get year-specific graph and node lists
  graph_yr <- main_g[[yr]]  # yearly graphs
  origins <- origin_list[[yr]]
  destinations <- destination_list[[yr]]
  
  # FILTER to nodes that exist in graph
  valid_origins <- origins[origins %in% V(graph_yr)$name]
  valid_destinations <- destinations[destinations %in% V(graph_yr)$name]
  
  # Skip if no valid nodes
  if(length(valid_origins) == 0 | length(valid_destinations) == 0) {
    return(tibble())
  }
  
  # Calculate distances
  dist_in_inv <- distances(
    graph_yr, 
    v = valid_origins, 
    to = valid_destinations, 
    weights = 1/E(graph_yr)$weight, 
    mode = "in"
  )
  
  dist_out_inv <- distances(
    graph_yr, 
    v = valid_origins, 
    to = valid_destinations, 
    weights = 1/E(graph_yr)$weight, 
    mode = "out"
  )
  
  # Calculate distances
  dist_in_raw <- distances(
    graph_yr, 
    v = valid_origins, 
    to = valid_destinations, 
    # weights = 1/E(graph_yr)$weight, 
    mode = "in"
  )
  
  dist_out_raw <- distances(
    graph_yr, 
    v = valid_origins, 
    to = valid_destinations, 
    # weights = 1/E(graph_yr)$weight, 
    mode = "out"
  )
  
  
  # Convert to data frame with all measures
  as.data.frame(dist_in_inv) %>%
    tibble::rownames_to_column("origin") %>%
    tidyr::pivot_longer(-origin, names_to = "destination", values_to = "wdist_in") %>%
    mutate(
      year = yr,
      wdist_out = as.data.frame(dist_out_inv) %>%
        tibble::rownames_to_column("origin") %>%
        tidyr::pivot_longer(-origin, names_to = "destination", values_to = "w_dist_out") %>%
        pull(w_dist_out),
      
      rawdist_in = as.data.frame(dist_in_raw) %>%
        tibble::rownames_to_column("origin") %>%
        tidyr::pivot_longer(-origin, names_to = "destination", values_to = "raw_dist_in") %>%
        pull(raw_dist_in),
      
      rawdist_out = as.data.frame(dist_out_raw) %>%
        tibble::rownames_to_column("origin") %>%
        tidyr::pivot_longer(-origin, names_to = "destination", values_to = "raw_dist_out") %>%
        pull(raw_dist_out)
    )
})

dist_minimal <- distances_by_year %>% 
  filter(origin != destination) %>% 
    pivot_longer(
      cols = c(wdist_in, wdist_out, rawdist_in, rawdist_out),
      names_to = c(".value", "direction"),
      names_pattern = "(.+)_(in|out)"
  ) %>%
  mutate(
    is_reach = if_else(is.infinite(wdist), 0, 1),
    across(
      c(wdist, rawdist),
      ~ ifelse(is.infinite(.x), NA, .x)
    )
  ) %>% 
    group_by(origin, destination, year) %>%
    slice_min(wdist, with_ties = T, na_rm = T) %>%
    ungroup() %>%
    group_by(destination, year) %>%
    slice_min(wdist, with_ties = T, na_rm = T)



  


# raw_data_long %>% 
#   mutate(cat = str_sub(tech, end = 1)) %>% 
#   rename(patent_n = value) %>% 
#   left_join(
#     # node_emb_own %>% 
#     nodes_emb_long %>% 
#       filter(metric == "oc") %>%
#       mutate(year = as.numeric(year)),
#     by = c("app_year" = "year", "tech" = "name", "cat")
#   ) %>% 
#   filter(is.na(value)) %>% count(tech)
#   group_by(app_year, region, cat, direction) %>% 
#   summarise(
#     value = sum(value, na.rm = T)
#   )


# regional_nodes <- tg %>% 
#   activate(nodes) %>% 
#   as_tibble() %>% 
#   filter(type == "regional") %>% 
#   pull(name) %>% unique()
# 
# potential_nodes <- tg %>% 
#   activate(nodes) %>% 
#   as_tibble() %>% 
#   filter(type == "potential") %>% 
#   pull(name) %>%  unique()
gc()

tg <- tg %>% 
  map(
    ~ .x %>% 
      activate(nodes) %>%
      # mutate(
      #   type = case_when(
      #     name %in% regional_nodes ~ "regional",
      #     name %in% potential_nodes ~ "potential",
      #     name %in% node_path_inter & name %in% potential_nodes ~ "intermediate potential",
      #     name %in% node_path_inter & !(name %in% potential_nodes) ~ "intermediate",
      #     T ~ "outsider"
      #   )
      # ) %>% 
      mutate(
        category = str_sub(name, end = 1),
        # technology = name,
        # category = category,  # uncomment if you have this attribute
        
        # Degree centralities
        degree_total = centrality_degree(mode = "total"), #(weights = weight),
        degree_in = centrality_degree(mode = "in"), #(mode = "in", weights = weight),
        degree_out = centrality_degree(mode = "out"), #(mode = "out", weights = weight),
        # 
        # Degree asymmetry (custom)
        # degree_asymmetry = degree_in / (degree_out + 0.001),  # +0.001 to avoid division by zero
        
        # Betweenness (weighted)
        betweenness = centrality_betweenness(directed = TRUE),
        
        # Closeness (weighted - inverts weights by default)
        # closeness_in = centrality_closeness(mode = "in"), # weights = weight),
        # closeness_out = centrality_closeness(mode = "out"), #, weights = weight),
        
        # Constraint (Burt's structural holes)
        burt_constraint = node_constraint(weights = weight),
        
        # Eigenvector centrality (weighted)
        eigenvector = centrality_eigen(weights = weight, directed = TRUE),
        
        # Authority and Hub scores (weighted)
        authority = centrality_authority(weights = weight),
        hub = centrality_hub(weights = weight)
      ) %>% 
      activate(edges) %>% 
      mutate(
        e_betweeness = centrality_edge_betweenness(directed = T)
      )
    
  )

reg_node_cat_data <- regional_data %>% 
  left_join(
    tg %>% 
      imap(
        ~ .x %>% 
          activate(nodes) %>% 
          as_tibble() %>% 
          mutate(year = as.numeric(.y))
      ) %>% 
      bind_rows(),
    by = c("year", "tech" = "name")
  ) %>% 
  mutate(
    country = str_sub(region2, end = 2)
  ) %>% 
  # select(-metric) %>% 
  left_join(
    GEI_data,
    by = c("year", "country" = "nuts0")
  ) %>% 
  left_join(
    dist_minimal %>% 
      ungroup() %>% 
      select(-origin) %>% 
      mutate(
        cat = str_sub(destination, end = 1),
        year = as.numeric(year)
      ),
    by = c("tech" = "destination", "year", "cat")
  ) 

reg_node_cat_data %>%  select(where(is.character)) %>%  colnames() -> char_names 


data_reg <- reg_node_cat_data %>%  
  filter(country %in% selected_countries) %>% 
  select(char_names, everything()) %>% 
  group_by(year) %>% 
  mutate(
    w_dist2 = 1/wdist,
    # ===== LOG + SCALE: Network metrics, counts, distance =====
    across(c(degree_total, degree_in, degree_out, betweenness, 
             att, abt, asp, gei, #hops removed
             patent_n, w_dist2, rawdist),
           ~scale(log(. + 1))[,1]),
    
    # ===== SCALE ONLY: Bounded centrality measures =====
    across(c(eigenvector, authority, hub, burt_constraint),
           ~scale(.)[,1]),
    
    # ===== SCALE ONLY: RCA and embededness (A-H categories) =====
    across(c(coherence, rca, 
             # A_to, A_from, B_to, B_from, C_to, C_from,
             # D_to, D_from, E_to, E_from, F_to, F_from,
             # G_to, G_from, H_to, H_from
             ),
           ~scale(.)[,1]),
    
    # ===== SCALE ONLY: GEI components and indices =====
    across(c(x1_opportunity_perception, x2_startup_skills, 
             x3_risk_acceptance, x4_networking, x5_cultural_support,
             x6_opportunity_startup, x7_technology_absorption, 
             x8_human_capital, x9_competition, x10_product_innovation,
             x11_process_innovation, x12_high_growth, 
             x13_internationalization, x14_risk_capital,
             individual, institutional),
           ~scale(.)[,1]),
    
  ) %>% 
  ungroup() %>% 
  group_by(tech, year) %>% 
  mutate(
    entry_adj = scale(pred1) %>%  as.numeric(),
    entry_adj = dplyr::percent_rank(entry_adj),
    entry_adj =  pmax(pmin(entry_adj, 0.999), 0.001)
  ) %>% 
  ungroup() 
  # mutate(
  #   region2 = case_when(
  #     # PORTUGAL MAPPING (old → new)
  #     region == "PT19" ~ "PT18",
  #     region == "PT1A" ~ "PT17", 
  #     region == "PT1B" ~ "PT18",
  #     region == "PT1C" ~ "PT18",
  #     region == "PT1D" ~ "PT18",
  #     
  #     # NETHERLANDS MAPPING (old → new)
  #     region == "NL35" ~ "NL33",
  #     region == "NL36" ~ "NL31",
  #     
  #     # UK MAPPING (NUTS 2013 → NUTS 2016) - VERIFIED
  #     # North East England
  #     region == "UKC3" ~ "UKC1",
  #     region == "UKC4" ~ "UKC2",
  #     
  #     # Eastern England  
  #     region == "UKH2" ~ "UKH1",
  #     region == "UKH3" ~ "UKH2",
  #     region == "UKH4" ~ "UKH3",
  #     region == "UKH5" ~ "UKH1",  # VERIFIED
  #     region == "UKH6" ~ "UKH1",  # VERIFIED
  #     
  #     # South West England
  #     region == "UKK3" ~ "UKK1",
  #     region == "UKK4" ~ "UKK2", 
  #     region == "UKK5" ~ "UKK3",
  #     region == "UKK6" ~ "UKK4",
  #     region == "UKK7" ~ "UKK1",  # VERIFIED
  #     
  #     # Wales
  #     region == "UKL3" ~ "UKL1",
  #     region == "UKL4" ~ "UKL2",
  #     region == "UKL5" ~ "UKL2",  # VERIFIED
  #     
  #     # Scotland
  #     region == "UKM0" ~ "UKM7",
  #     region == "UKM1" ~ "UKM5",
  #     region == "UKM2" ~ "UKM7",  
  #     region == "UKM3" ~ "UKM9",
  #     
  #     # Keep all other regions unchanged
  #     TRUE ~ region
  #   )
  # )
  
# data_reg %>% write_csv(here::here("data", "data_reg.csv"))

pacman::p_load(
  patchwork, ggraph, tidygraph, networkD3, lsa, igraph, mgcv, 
  spdep, regions, spaMM, splm, spatialreg, HSAR,
  tidyverse, targets, gt,  sf, ggiraph, lme4, brms, glmmTMB, eurostat
)


data_reg <- data_reg %>% #read_csv(here::here("data", "data_reg.csv")) %>%
  mutate(
    entry_adj_b = if_else(entry_adj >= .5, 1, 0),
    tech = as.factor(tech),
    country = as.factor(country),
    regionf = as.factor(region2),
    year = as.factor(year),  # Important if using as random effect
    region_in_country = interaction(region2, country, drop = TRUE)
  )



sf_u <- sf_data %>% pull(NUTS_ID) %>%  unique()

dr_u <- data_reg %>% pull(region2) %>%  unique()

dr_u[!(dr_u %in% sf_u)] 

sf_data_nb <- sf_data %>% 
  filter(NUTS_ID %in% dr_u) %>% 
  arrange(NUTS_ID)


# reg_data_sf <- data_reg %>% 
#   left_join(sf_data, by = c("region2" = "NUTS_ID", "country" = "CNTR_CODE")) %>% 
#   st_as_sf()


# Create neighbors from sf_data
nb <- poly2nb(sf_data_nb, queen = TRUE)

listw <- nb2listw(nb, style = "W", zero.policy = TRUE)

# Add region names
# rownames(adj_mat) <- sf_data_nb$NUTS_ID
# colnames(adj_mat) <- sf_data_nb$NUTS_ID

form <- entry_adj ~ 
  patent_n + 
  degree_in + degree_out +
  betweenness + 
  authority + 
  hub + 
  eigenvector +
  burt_constraint +
  rawdist + coherence + gei + cat


m0 <- hsar(
  formula = form,
  data = data_reg %>%  filter(year == 2018) %>% arrange(region),
  listw = listw
)
# reg_data_sf %>% glimpse()

# data_prep <- paths_data_minimal %>%
#   ungroup() %>%
#   # select(-path, -dist) %>%
#   separate_longer_delim(node_path, delim = ",") %>%
#   mutate(country = str_sub(region, end = 2)) %>% 
#   select()
#   mutate(
#     nuts0 = str_sub(region, end = 2),
#     type = case_when(
#       node_path == origin ~ "regional",
#       node_path == destination ~ "potential",
#       node_path != origin & node_path != destination ~ "intermediate",
#       T ~ "other"
#     ),
#     entry = case_when(
#       type == "regional" ~ 0,
#       type == "potential" ~ 1,
#       T ~ NA
#     ),
#     direct = if_else(hops == 1, "direct_link", "indirect_link", missing = "other")
#   ) %>%
#   left_join(
#     tg %>%
#       map_dfr(~ .x %>% activate(nodes) %>% as_tibble(), .id = "year") %>%
#       mutate(year = as.numeric(year)),
#     by = c("year", "node_path" = "name")
#   ) %>%
#   left_join(
#     GEI_data
#   ) %>%
#   mutate(
#     across(
#       c(raw_dist, gei, institutional, degree_total:hub),
#       ~ scale(.x) %>%  as.numeric()
#     )
#   )
# 
# data_reg %>% 
#   select(where(is.numeric)) %>% 
#   modelsummary::datasummary_correlation()
# 
m1 <- glmer(
  entry_adj_b ~ 
    patent_n + 
    degree_in + degree_out +
    betweenness + 
    authority + 
    hub + 
    eigenvector +
    burt_constraint +
    rawdist + coherence + gei + cat + 
    # (1 | ) + factor(year) +
    (1 | year) +
    (1 | category/tech) +
    (1 | country/region),
  family = binomial(link = "logit"),
  data = data_reg #, REML = T
)

# 
# summary(m1)
# 
# # Model WITHOUT technology control
m11 <-  lm(entry_adj ~
             coherence*patent_n +
             coherence*degree_total +
             coherence*betweenness +
             authority +  hub +
             coherence*eigenvector +
             coherence*burt_constraint +
                  gei +
             coherence*wdist + 
             year + tech + region + country,
              data = data_reg)

summary(m11)


m_spatial <- glmmTMB(
  entry_adj ~ 
    # Technology network properties (from paragraph 3-4 of typical economic complexity papers)
    degree_in + degree_out + betweenness + 
    eigenvector + authority + hub + 
    burt_constraint + coherence +
    
    # Distance and embeddedness
    rawdist + 
    
    # Regional capabilities
    patent_n + gei + 
    
    # Random effects hierarchy
    (1 | tech) +                    # Technology-specific baseline
    (1 | category) +                # Technology category effects
    (1 | year) +                    # Temporal effects
    (1 | country) +                 # Country-level effects
    (1 | region),                   # SPATIAL region effects
  control = glmmTMBControl(parallel = list(n = 4L, autopar = TRUE)),
  family = beta_family(link = "logit"),
  data = data_reg
)

# Model WITH technology control
# this is our guy!

options(glmmTMB_openmp_debug = TRUE)

m22 <- glmmTMB(entry_adj ~ 
                 patent_n + 
                 degree_in + degree_out +
                 betweenness + 
                 authority + 
                 hub + 
                 eigenvector +
                 burt_constraint +
                 rawdist + coherence + gei + cat + 
                 (1 | tech) + #factor(year) +
                 (1 | category) +
                 (1 | country/region),
               # (1 | country/region + year),
               family = beta_family(link = "logit"),
               control = glmmTMBControl(parallel = list(n = 8L, autopar = TRUE)),
               data = data_reg %>% filter(year == 2018))

summary(m22)

m_fast <- bam(
  entry_adj ~ 
    patent_n +
    # coherence*degree_total +
    coherence*betweenness +
    # authority +  hub +
    coherence*authority +
    coherence*hub +
    coherence*rawdist + 
    # coherence*burt_constraint +
    x8_human_capital + x9_competition + x10_product_innovation + 
    x11_process_innovation + x12_high_growth + x13_internationalization + 
    x14_risk_capital +
    cat +
    s(year, bs = "re") + 
    s(region2, bs = "re") +
    s(tech, bs="re") + #s(category, bs = "re") +
    s(country, bs="re"),
    # s(region, country, bs = "re"),
    # s(region_in_country, bs="re"),
  family = betar(), data = data_reg, 
  discrete = TRUE, nthreads = 10
)

summary(m_fast)

# Compare
# anova(m11, m22)
# library(brms)
# Enable debugging to see what's happening
m_bayes <- brm(
  entry_adj ~ 
    patent_n + 
    degree_in + degree_out +
    betweenness + 
    authority + 
    hub + 
    eigenvector +
    burt_constraint +
    rawdist + coherence + gei + cat + 
    (1 | year) + (1 | tech) + (1 | country) + (1 | region),
  
  family = Beta(link = "logit"),
  
  # Weakly informative priors
  # prior = c(
  #   prior(normal(0, 2.5), class = "b"),      # Fixed effects (~ uniform on probability scale)
  #   prior(student_t(3, 0, 2.5), class = "sd"),  # Random effects SDs (weakly informative)
  #   prior(gamma(0.01, 0.01), class = "phi")  # Beta dispersion (vague)
  # ),
  
  data = data_reg,
  # algorithm = "meanfield",
  # MCMC settings
  chains = 4,
  cores = 4,
  iter = 1000, warmup = 500,
  threads = threading(2),
  # Use cmdstanr (faster than rstan)
  backend = "cmdstanr"
  
  # Speed optimizations
  # control = list(adapt_delta = 0.95, max_treedepth = 12),

)

# Summary
summary(m_bayes)
# Test all negative/positive hypotheses
hypothesis(m1_bayes, c(
  "hops:raw_dist < 0",           # Distance reduces entry
  "degree_total < 0",       # Centrality reduces entry
  "betweenness > 0",        # Bridging increases entry
  "eigenvector < 0",        # Elite status reduces entry
  "burt_constraint < 0"
))


summary(m1_bayes)

summary(m2_bayes)

pp_check(m2_bayes, type = "dens_overlay")

conditional_effects(m2_bayes, "betweenness:category")

# Compute LOO for each model
loo1 <- loo(m1_bayes)
loo2 <- loo(m2_bayes)

# WAIC can compare models with different N
waic1 <- waic(m1_bayes)
waic2 <- waic(m2_bayes)

# Compare
loo_compare(loo1, loo2)

bayes_R2(m1_bayes)
bayes_R2(m2_bayes)
